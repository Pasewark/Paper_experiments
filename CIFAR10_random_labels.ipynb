{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10-random_labels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP3mCzOqeTKhwYUIoW28H1s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasewark/Paper_experiments/blob/main/CIFAR10_random_labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTywhked5Sw0"
      },
      "source": [
        "#train a network on random labels\n",
        "#similar to experiments from https://arxiv.org/abs/1611.03530\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import PIL\n",
        "import torchvision.models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device=torch.device('cuda')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAYOD05x6OOn",
        "outputId": "a70e9234-edba-4795-e8a2-48471799c865"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7vLAbt3dXJG"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSnGAv4OYOEB",
        "outputId": "44d63a1b-bf2b-4284-d39e-6a7233eedee0"
      },
      "source": [
        "model=torchvision.models.resnet50()\n",
        "print(model) #note that the last layer has dim 1000 output"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on_xSIQdYbg6",
        "outputId": "d095bcfa-c080-441a-94d5-9d30891cf305"
      },
      "source": [
        "model.fc=nn.Linear(2048,10)\n",
        "print(model) #note that the last layer now has dim 10 output"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5KeNyzv6j9b"
      },
      "source": [
        "model=model.to(device)\n",
        "optimizer=optim.SGD(model.parameters(),lr=.01)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=5,mode='max',verbose=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4MDGxoFeb7p",
        "outputId": "aeaf3dac-c2d2-4dca-edd2-1e88af60e973"
      },
      "source": [
        "#train on normal data for comparison\n",
        "losses=[]\n",
        "accs=[]\n",
        "losses_val=[]\n",
        "accs_val=[]\n",
        "for epoch in range(100):\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    running_loss_val=0\n",
        "    running_acc_val=0\n",
        "    model.train()\n",
        "    for i,batch in enumerate(trainloader):\n",
        "        images,labels=batch[0].to(device),batch[1].to(device)\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        preds=outputs.argmax(1)\n",
        "        running_acc+=torch.sum(preds==labels).item()/len(labels)\n",
        "    model.eval()\n",
        "    for i,batch in enumerate(testloader):\n",
        "        images,labels=batch[0].to(device),batch[1].to(device)\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        running_loss_val+=loss.item()\n",
        "        preds=outputs.argmax(1)\n",
        "        running_acc_val+=torch.sum(preds==labels).item()/len(labels)\n",
        "    print('epoch',epoch)\n",
        "    print('train_loss',running_loss/len(trainloader),'train_acc',running_acc/len(trainloader))\n",
        "    print('val_loss',running_loss_val/len(testloader),'val_acc',running_acc_val/len(testloader))\n",
        "    losses.append(running_loss/len(trainloader))\n",
        "    accs.append(running_acc/len(trainloader))\n",
        "    losses_val.append(running_loss_val/len(testloader))\n",
        "    accs_val.append(running_acc_val/len(testloader))\n",
        "    scheduler.step(running_acc/len(trainloader))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "train_loss 2.3105652344501233 train_acc 0.22590712915601024\n",
            "val_loss 2.147499508495572 val_acc 0.2813488924050633\n",
            "epoch 1\n",
            "train_loss 1.8542506508815013 train_acc 0.3476262787723785\n",
            "val_loss 1.80747830264176 val_acc 0.37005537974683544\n",
            "epoch 2\n",
            "train_loss 1.6407674380275599 train_acc 0.4125\n",
            "val_loss 1.5349228276482112 val_acc 0.4459058544303797\n",
            "epoch 3\n",
            "train_loss 1.520460400130133 train_acc 0.4535925511508952\n",
            "val_loss 1.565788949592204 val_acc 0.44343354430379744\n",
            "epoch 4\n",
            "train_loss 1.426467597027264 train_acc 0.48751998081841436\n",
            "val_loss 1.4688228459297856 val_acc 0.47379351265822783\n",
            "epoch 5\n",
            "train_loss 1.3486943949214028 train_acc 0.513650895140665\n",
            "val_loss 1.3995143476920793 val_acc 0.49535205696202533\n",
            "epoch 6\n",
            "train_loss 1.2896521650921657 train_acc 0.5369884910485934\n",
            "val_loss 1.3457690341563164 val_acc 0.5146360759493671\n",
            "epoch 7\n",
            "train_loss 1.2282960091710395 train_acc 0.558667679028133\n",
            "val_loss 1.3471631490731542 val_acc 0.5133504746835443\n",
            "epoch 8\n",
            "train_loss 1.1726447828590412 train_acc 0.58198929028133\n",
            "val_loss 1.3205810521222368 val_acc 0.5291732594936709\n",
            "epoch 9\n",
            "train_loss 1.11464988865206 train_acc 0.6016184462915601\n",
            "val_loss 1.304195869572555 val_acc 0.5381724683544303\n",
            "epoch 10\n",
            "train_loss 1.0665957699041537 train_acc 0.6203644501278772\n",
            "val_loss 1.3014440657217292 val_acc 0.5447982594936709\n",
            "epoch 11\n",
            "train_loss 1.0136501377500842 train_acc 0.6398737212276215\n",
            "val_loss 1.2943412050416199 val_acc 0.547567246835443\n",
            "epoch 12\n",
            "train_loss 0.9691965860479018 train_acc 0.6567695012787724\n",
            "val_loss 1.2790267890012716 val_acc 0.5589398734177216\n",
            "epoch 13\n",
            "train_loss 0.9268987796190754 train_acc 0.6723025895140665\n",
            "val_loss 1.3106585507151447 val_acc 0.5521162974683544\n",
            "epoch 14\n",
            "train_loss 0.8819745109819085 train_acc 0.6871363491048594\n",
            "val_loss 1.2596392835242838 val_acc 0.5668512658227848\n",
            "epoch 15\n",
            "train_loss 0.8384320690198932 train_acc 0.7036325127877238\n",
            "val_loss 1.3367319363581984 val_acc 0.5556764240506329\n",
            "epoch 16\n",
            "train_loss 0.7995891284454814 train_acc 0.7202046035805627\n",
            "val_loss 1.2713829535472243 val_acc 0.5869264240506329\n",
            "epoch 17\n",
            "train_loss 0.7565647720375939 train_acc 0.7339114450127878\n",
            "val_loss 1.2769441815871228 val_acc 0.5789161392405063\n",
            "epoch 18\n",
            "train_loss 0.7237836980758725 train_acc 0.7438858695652174\n",
            "val_loss 1.3638923952851114 val_acc 0.5709058544303798\n",
            "epoch 19\n",
            "train_loss 0.6912327488060193 train_acc 0.7571491368286446\n",
            "val_loss 1.3607783053494706 val_acc 0.5745648734177216\n",
            "epoch 20\n",
            "train_loss 0.6510384880825687 train_acc 0.7693294437340154\n",
            "val_loss 1.3080676960039743 val_acc 0.5907832278481012\n",
            "epoch 21\n",
            "train_loss 0.617722243909031 train_acc 0.7845188618925831\n",
            "val_loss 1.3655115109455735 val_acc 0.5793117088607594\n",
            "epoch 22\n",
            "train_loss 0.5860913702289162 train_acc 0.7937300191815857\n",
            "val_loss 1.3833404213567324 val_acc 0.5847507911392406\n",
            "epoch 23\n",
            "train_loss 0.5508044244688185 train_acc 0.8065217391304348\n",
            "val_loss 1.4201242516312418 val_acc 0.5845530063291139\n",
            "epoch 24\n",
            "train_loss 0.5098897405826223 train_acc 0.822494405370844\n",
            "val_loss 1.452395836009255 val_acc 0.5865308544303798\n",
            "epoch 25\n",
            "train_loss 0.491777815324876 train_acc 0.8274816176470589\n",
            "val_loss 1.4467472207697132 val_acc 0.592068829113924\n",
            "epoch 26\n",
            "train_loss 0.4600960989208782 train_acc 0.8391903772378517\n",
            "val_loss 1.4614510687091682 val_acc 0.5947389240506329\n",
            "epoch 27\n",
            "train_loss 0.4274515291613996 train_acc 0.8493366368286445\n",
            "val_loss 1.5401361471490016 val_acc 0.5812895569620253\n",
            "epoch 28\n",
            "train_loss 0.4064399083252148 train_acc 0.8584558823529411\n",
            "val_loss 1.563176060024696 val_acc 0.5897943037974683\n",
            "epoch 29\n",
            "train_loss 0.38832218129464124 train_acc 0.8636109335038363\n",
            "val_loss 1.5453590584706656 val_acc 0.5873219936708861\n",
            "epoch 30\n",
            "train_loss 0.36011704684370927 train_acc 0.8732217071611253\n",
            "val_loss 1.5847180719617047 val_acc 0.5874208860759493\n",
            "epoch 31\n",
            "train_loss 0.34533722416671647 train_acc 0.8780290920716112\n",
            "val_loss 1.554637450960618 val_acc 0.5887064873417721\n",
            "epoch 32\n",
            "train_loss 0.32649844797218547 train_acc 0.8855778452685422\n",
            "val_loss 1.6104064636592623 val_acc 0.5956289556962026\n",
            "epoch 33\n",
            "train_loss 0.2984295117733119 train_acc 0.8948129795396419\n",
            "val_loss 1.680525328539595 val_acc 0.5981012658227848\n",
            "epoch 34\n",
            "train_loss 0.281882536075914 train_acc 0.9032009271099745\n",
            "val_loss 1.6581638312037987 val_acc 0.6051226265822784\n",
            "epoch 35\n",
            "train_loss 0.26865531043018526 train_acc 0.9051190856777493\n",
            "val_loss 1.721879827825329 val_acc 0.6010680379746836\n",
            "epoch 36\n",
            "train_loss 0.24823258064515755 train_acc 0.9130035166240409\n",
            "val_loss 1.74708843382099 val_acc 0.5949367088607594\n",
            "epoch 37\n",
            "train_loss 0.23143471863187487 train_acc 0.9175351662404093\n",
            "val_loss 1.813747270197808 val_acc 0.6007713607594937\n",
            "epoch 38\n",
            "train_loss 0.22726000471950492 train_acc 0.920576246803069\n",
            "val_loss 1.7155480324467527 val_acc 0.6149129746835443\n",
            "epoch 39\n",
            "train_loss 0.21072808908455817 train_acc 0.9259111253196931\n",
            "val_loss 1.7991889926451672 val_acc 0.6105617088607594\n",
            "epoch 40\n",
            "train_loss 0.2056972122825015 train_acc 0.9279052109974425\n",
            "val_loss 1.8546741008758545 val_acc 0.6072982594936709\n",
            "epoch 41\n",
            "train_loss 0.19411098877982716 train_acc 0.9323249680306905\n",
            "val_loss 1.8196504915816873 val_acc 0.6088805379746836\n",
            "epoch 42\n",
            "train_loss 0.17685234840111355 train_acc 0.9377797314578005\n",
            "val_loss 1.9086620294595067 val_acc 0.6073971518987342\n",
            "epoch 43\n",
            "train_loss 0.17230277830530005 train_acc 0.9392942774936062\n",
            "val_loss 1.9771466406085823 val_acc 0.5932555379746836\n",
            "epoch 44\n",
            "train_loss 0.16805396138516535 train_acc 0.9409047314578005\n",
            "val_loss 1.8991485607774952 val_acc 0.6073971518987342\n",
            "epoch 45\n",
            "train_loss 0.15004203267528882 train_acc 0.9466871803069055\n",
            "val_loss 1.9293403595308714 val_acc 0.6144185126582279\n",
            "epoch 46\n",
            "train_loss 0.14705407833847242 train_acc 0.9485014386189258\n",
            "val_loss 1.9547134139869786 val_acc 0.6150118670886076\n",
            "epoch 47\n",
            "train_loss 0.14177640292155164 train_acc 0.9502997122762148\n",
            "val_loss 1.9825278611122807 val_acc 0.6120450949367089\n",
            "epoch 48\n",
            "train_loss 0.14306927109351547 train_acc 0.9493166560102302\n",
            "val_loss 1.9610221929188016 val_acc 0.6160007911392406\n",
            "epoch 49\n",
            "train_loss 0.12692962666911542 train_acc 0.954895300511509\n",
            "val_loss 2.0065724110301537 val_acc 0.6161985759493671\n",
            "epoch 50\n",
            "train_loss 0.12503899269930238 train_acc 0.9554227941176471\n",
            "val_loss 1.9871130547946012 val_acc 0.6172863924050633\n",
            "epoch 51\n",
            "train_loss 0.11628857146367393 train_acc 0.9594309462915601\n",
            "val_loss 2.0473206677014315 val_acc 0.6099683544303798\n",
            "epoch 52\n",
            "train_loss 0.11813697776735743 train_acc 0.9587515984654731\n",
            "val_loss 1.9926083661332916 val_acc 0.6199564873417721\n",
            "epoch 53\n",
            "train_loss 0.10737451453171575 train_acc 0.9615768861892583\n",
            "val_loss 2.083805734598184 val_acc 0.6179786392405063\n",
            "epoch 54\n",
            "train_loss 0.10535856430916607 train_acc 0.9633391943734014\n",
            "val_loss 2.0286646885207937 val_acc 0.6224287974683544\n",
            "epoch 55\n",
            "train_loss 0.09962701697921966 train_acc 0.9645100703324809\n",
            "val_loss 2.01292605641522 val_acc 0.6327136075949367\n",
            "epoch 56\n",
            "train_loss 0.10311248927565335 train_acc 0.9640265345268542\n",
            "val_loss 2.094253205045869 val_acc 0.6190664556962026\n",
            "epoch 57\n",
            "train_loss 0.09806482397172304 train_acc 0.9650615409207162\n",
            "val_loss 2.1328860234610643 val_acc 0.614121835443038\n",
            "epoch 58\n",
            "train_loss 0.09032740560181611 train_acc 0.9684502877237852\n",
            "val_loss 2.1131460576117793 val_acc 0.6264833860759493\n",
            "epoch 59\n",
            "train_loss 0.08746702016314582 train_acc 0.9689737851662403\n",
            "val_loss 2.143823782099953 val_acc 0.621934335443038\n",
            "epoch 60\n",
            "train_loss 0.08794842679720477 train_acc 0.9687739769820971\n",
            "val_loss 2.1448247530792335 val_acc 0.6298457278481012\n",
            "epoch 61\n",
            "train_loss 0.0793338615442519 train_acc 0.972993925831202\n",
            "val_loss 2.1524119165879263 val_acc 0.6220332278481012\n",
            "epoch 62\n",
            "train_loss 0.08279343442205349 train_acc 0.9709958439897699\n",
            "val_loss 2.218294742741162 val_acc 0.6182753164556962\n",
            "epoch 63\n",
            "train_loss 0.0803615925452479 train_acc 0.9715153452685422\n",
            "val_loss 2.1968501760989807 val_acc 0.6218354430379747\n",
            "epoch 64\n",
            "train_loss 0.07748804814980158 train_acc 0.9726462595907929\n",
            "val_loss 2.188587386396867 val_acc 0.6237143987341772\n",
            "epoch 65\n",
            "train_loss 0.06623399547298851 train_acc 0.976974104859335\n",
            "val_loss 2.257983471773848 val_acc 0.6121439873417721\n",
            "epoch 66\n",
            "train_loss 0.07026158096364049 train_acc 0.9748681265984654\n",
            "val_loss 2.2611644826357877 val_acc 0.6203520569620253\n",
            "epoch 67\n",
            "train_loss 0.0705273397948088 train_acc 0.9753396739130435\n",
            "val_loss 2.229786860791943 val_acc 0.627373417721519\n",
            "epoch 68\n",
            "train_loss 0.062012027347665234 train_acc 0.9789362212276216\n",
            "val_loss 2.292900800704956 val_acc 0.6169897151898734\n",
            "epoch 69\n",
            "train_loss 0.06537704417944107 train_acc 0.977417679028133\n",
            "val_loss 2.237056751794453 val_acc 0.6261867088607594\n",
            "epoch 70\n",
            "train_loss 0.057526518232272485 train_acc 0.9796395460358056\n",
            "val_loss 2.2135574349874183 val_acc 0.623318829113924\n",
            "epoch 71\n",
            "train_loss 0.05434450156667539 train_acc 0.9813219309462915\n",
            "val_loss 2.361506772946708 val_acc 0.6172863924050633\n",
            "epoch 72\n",
            "train_loss 0.05653543667534314 train_acc 0.9802070012787724\n",
            "val_loss 2.257843747923646 val_acc 0.6303401898734177\n",
            "epoch 73\n",
            "train_loss 0.0542309180133121 train_acc 0.9813219309462915\n",
            "val_loss 2.3076992095271245 val_acc 0.6245055379746836\n",
            "epoch 74\n",
            "train_loss 0.04654805014586395 train_acc 0.9833519820971868\n",
            "val_loss 2.308092665068711 val_acc 0.6279667721518988\n",
            "epoch 75\n",
            "train_loss 0.05232942149119304 train_acc 0.9823209718670076\n",
            "val_loss 2.2748482046248037 val_acc 0.6310324367088608\n",
            "epoch 76\n",
            "train_loss 0.05095115937102024 train_acc 0.9822890025575447\n",
            "val_loss 2.279254723198806 val_acc 0.6330102848101266\n",
            "epoch 77\n",
            "train_loss 0.049497315187907544 train_acc 0.9823649296675192\n",
            "val_loss 2.30583362488807 val_acc 0.6347903481012658\n",
            "epoch 78\n",
            "train_loss 0.0541204853350168 train_acc 0.9818534207161126\n",
            "val_loss 2.263151718091361 val_acc 0.6261867088607594\n",
            "epoch 79\n",
            "train_loss 0.051237739341528823 train_acc 0.9822929987212277\n",
            "val_loss 2.294772099845017 val_acc 0.6348892405063291\n",
            "epoch 80\n",
            "train_loss 0.04742625325232211 train_acc 0.9830722506393863\n",
            "val_loss 2.3711879615542255 val_acc 0.6229232594936709\n",
            "Epoch    81: reducing learning rate of group 0 to 1.0000e-03.\n",
            "epoch 81\n",
            "train_loss 0.030995957256244766 train_acc 0.9894101662404092\n",
            "val_loss 2.313595945322061 val_acc 0.6324169303797469\n",
            "epoch 82\n",
            "train_loss 0.02341755373073537 train_acc 0.9926430626598466\n",
            "val_loss 2.2690144882926457 val_acc 0.638943829113924\n",
            "epoch 83\n",
            "train_loss 0.0201867015504152 train_acc 0.9934902493606137\n",
            "val_loss 2.2576549928399583 val_acc 0.6386471518987342\n",
            "epoch 84\n",
            "train_loss 0.016343436754711182 train_acc 0.9950047953964194\n",
            "val_loss 2.2696585323237164 val_acc 0.6357792721518988\n",
            "epoch 85\n",
            "train_loss 0.01459005001969302 train_acc 0.9955442774936062\n",
            "val_loss 2.287604677526257 val_acc 0.6343947784810127\n",
            "epoch 86\n",
            "train_loss 0.013206121455336138 train_acc 0.9959838554987213\n",
            "val_loss 2.263571686382535 val_acc 0.6433939873417721\n",
            "epoch 87\n",
            "train_loss 0.012439142006909107 train_acc 0.996403452685422\n",
            "val_loss 2.3170040031022663 val_acc 0.6387460443037974\n",
            "epoch 88\n",
            "train_loss 0.012004868109094794 train_acc 0.996611253196931\n",
            "val_loss 2.2878049989289875 val_acc 0.6443829113924051\n",
            "epoch 89\n",
            "train_loss 0.010496252113977051 train_acc 0.9971627237851662\n",
            "val_loss 2.2933576152294495 val_acc 0.6379549050632911\n",
            "epoch 90\n",
            "train_loss 0.011096246151105903 train_acc 0.9970108695652175\n",
            "val_loss 2.292729623710053 val_acc 0.6391416139240507\n",
            "epoch 91\n",
            "train_loss 0.01079401104515085 train_acc 0.9973225703324808\n",
            "val_loss 2.331295380109473 val_acc 0.6372626582278481\n",
            "epoch 92\n",
            "train_loss 0.00863045473988561 train_acc 0.997582320971867\n",
            "val_loss 2.3260779441157475 val_acc 0.6403283227848101\n",
            "epoch 93\n",
            "train_loss 0.008501497520547113 train_acc 0.9977181905370844\n",
            "val_loss 2.281054354921172 val_acc 0.637559335443038\n",
            "epoch 94\n",
            "train_loss 0.008050985718512184 train_acc 0.9978420716112532\n",
            "val_loss 2.3207724305647837 val_acc 0.6416139240506329\n",
            "epoch 95\n",
            "train_loss 0.008202189808268376 train_acc 0.9976622442455243\n",
            "val_loss 2.325756607176382 val_acc 0.6366693037974683\n",
            "epoch 96\n",
            "train_loss 0.007625049144095715 train_acc 0.9981098145780052\n",
            "val_loss 2.3299061573004423 val_acc 0.6383504746835443\n",
            "epoch 97\n",
            "train_loss 0.008271538070626342 train_acc 0.9977221867007673\n",
            "val_loss 2.347596755510644 val_acc 0.6380537974683544\n",
            "epoch 98\n",
            "train_loss 0.0068664364332137896 train_acc 0.9982217071611253\n",
            "val_loss 2.3615777975396264 val_acc 0.6395371835443038\n",
            "epoch 99\n",
            "train_loss 0.006861318154838127 train_acc 0.9982217071611253\n",
            "val_loss 2.3076771527906006 val_acc 0.6402294303797469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJJJmuRjZnPJ"
      },
      "source": [
        "#save our losses to compare later\n",
        "losses_old=losses\n",
        "accs_old=accs\n",
        "losses_old_val=losses_val\n",
        "accs_old_val=accs_val"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9z7QGNbd4EM"
      },
      "source": [
        "trainset.targets=np.random.randint(0,10,len(trainset.targets)).tolist() #change the labels to random"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCGgCcZanLtb"
      },
      "source": [
        "model=torchvision.models.resnet50()\n",
        "model.fc=nn.Linear(2048,10)\n",
        "model=model.to(device)\n",
        "optimizer=optim.SGD(model.parameters(),lr=.01)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "scheduler=optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=5,mode='max',verbose=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmrRk48I69_C",
        "outputId": "2cd144e6-aa2e-49b1-f230-52df9f2f1aec"
      },
      "source": [
        "#train on data with random labels\n",
        "losses=[]\n",
        "accs=[]\n",
        "losses_val=[]\n",
        "accs_val=[]\n",
        "for epoch in range(250):\n",
        "    running_loss=0\n",
        "    running_acc=0\n",
        "    running_loss_val=0\n",
        "    running_acc_val=0\n",
        "    model.train()\n",
        "    for i,batch in enumerate(trainloader):\n",
        "        images,labels=batch[0].to(device),batch[1].to(device)\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss+=loss.item()\n",
        "        preds=outputs.argmax(1)\n",
        "        running_acc+=torch.sum(preds==labels).item()/len(labels)\n",
        "    model.eval()\n",
        "    for i,batch in enumerate(testloader):\n",
        "        images,labels=batch[0].to(device),batch[1].to(device)\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        running_loss_val+=loss.item()\n",
        "        preds=outputs.argmax(1)\n",
        "        running_acc_val+=torch.sum(preds==labels).item()/len(labels)\n",
        "    print('epoch',epoch)\n",
        "    print('train_loss',running_loss/len(trainloader),'train_acc',running_acc/len(trainloader))\n",
        "    print('val_loss',running_loss_val/len(testloader),'val_acc',running_acc_val/len(testloader)) #acc should be about .1 with random labels\n",
        "    losses.append(running_loss/len(trainloader))\n",
        "    accs.append(running_acc/len(trainloader))\n",
        "    losses_val.append(running_loss_val/len(testloader))\n",
        "    accs_val.append(running_acc_val/len(testloader))\n",
        "    scheduler.step(running_acc/len(trainloader))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n",
            "train_loss 2.7303870724290227 train_acc 0.09831761508951407\n",
            "val_loss 2.6262541783006887 val_acc 0.09177215189873418\n",
            "epoch 1\n",
            "train_loss 2.4908861876143824 train_acc 0.09930866368286445\n",
            "val_loss 2.4630014081544513 val_acc 0.10393591772151899\n",
            "epoch 2\n",
            "train_loss 2.412728813298218 train_acc 0.10172634271099744\n",
            "val_loss 2.399237560320504 val_acc 0.10561708860759493\n",
            "epoch 3\n",
            "train_loss 2.3750888411041418 train_acc 0.10313299232736574\n",
            "val_loss 2.44603903685944 val_acc 0.09671677215189874\n",
            "epoch 4\n",
            "train_loss 2.3565025823500454 train_acc 0.10629395780051151\n",
            "val_loss 2.4033729728264146 val_acc 0.09978243670886076\n",
            "epoch 5\n",
            "train_loss 2.34212088889783 train_acc 0.10924312659846547\n",
            "val_loss 2.3533679895763155 val_acc 0.09918908227848101\n",
            "epoch 6\n",
            "train_loss 2.3323726428439246 train_acc 0.11295156649616368\n",
            "val_loss 2.3912968424302115 val_acc 0.10888053797468354\n",
            "epoch 7\n",
            "train_loss 2.3243168961361547 train_acc 0.11568893861892583\n",
            "val_loss 2.3720199156411086 val_acc 0.09572784810126582\n",
            "epoch 8\n",
            "train_loss 2.3204340605479676 train_acc 0.12105978260869565\n",
            "val_loss 2.3620405891273597 val_acc 0.09068433544303797\n",
            "epoch 9\n",
            "train_loss 2.3144597531584523 train_acc 0.12394101662404092\n",
            "val_loss 2.4171554710291607 val_acc 0.09523338607594936\n",
            "epoch 10\n",
            "train_loss 2.3101488836586017 train_acc 0.12698209718670075\n",
            "val_loss 2.35294934767711 val_acc 0.09760680379746836\n",
            "epoch 11\n",
            "train_loss 2.30394239193948 train_acc 0.13134990409207162\n",
            "val_loss 2.337199117563948 val_acc 0.10561708860759493\n",
            "epoch 12\n",
            "train_loss 2.296100435964287 train_acc 0.13889466112531967\n",
            "val_loss 2.3542268095137198 val_acc 0.10245253164556962\n",
            "epoch 13\n",
            "train_loss 2.2924368058324167 train_acc 0.1400975063938619\n",
            "val_loss 2.366487243507482 val_acc 0.10314477848101265\n",
            "epoch 14\n",
            "train_loss 2.2852935266616705 train_acc 0.145548273657289\n",
            "val_loss 2.380466138260274 val_acc 0.10324367088607594\n",
            "epoch 15\n",
            "train_loss 2.280565649042349 train_acc 0.15235374040920716\n",
            "val_loss 2.3728267241127883 val_acc 0.09879351265822785\n",
            "epoch 16\n",
            "train_loss 2.2731211270822587 train_acc 0.15813618925831202\n",
            "val_loss 2.387122193469277 val_acc 0.10749604430379747\n",
            "epoch 17\n",
            "train_loss 2.264993148691514 train_acc 0.16359894501278774\n",
            "val_loss 2.3750487792341013 val_acc 0.09879351265822785\n",
            "epoch 18\n",
            "train_loss 2.260319031108066 train_acc 0.1658048273657289\n",
            "val_loss 2.395502615578567 val_acc 0.1071004746835443\n",
            "epoch 19\n",
            "train_loss 2.2491162085472167 train_acc 0.17130354859335037\n",
            "val_loss 2.389827151841755 val_acc 0.09780458860759493\n",
            "epoch 20\n",
            "train_loss 2.238555994180157 train_acc 0.17952365728900255\n",
            "val_loss 2.425690813909603 val_acc 0.10096914556962025\n",
            "epoch 21\n",
            "train_loss 2.2265242243666785 train_acc 0.18720428388746804\n",
            "val_loss 2.456736232660994 val_acc 0.09187104430379747\n",
            "epoch 22\n",
            "train_loss 2.217020471382629 train_acc 0.19145220588235293\n",
            "val_loss 2.433728314653228 val_acc 0.10096914556962025\n",
            "epoch 23\n",
            "train_loss 2.2012280978815024 train_acc 0.19873321611253197\n",
            "val_loss 2.4647900001912175 val_acc 0.09958465189873418\n",
            "epoch 24\n",
            "train_loss 2.1847022537075347 train_acc 0.2088914641943734\n",
            "val_loss 2.4697184230707867 val_acc 0.10512262658227849\n",
            "epoch 25\n",
            "train_loss 2.166817819980709 train_acc 0.21962915601023017\n",
            "val_loss 2.495208381097528 val_acc 0.10116693037974683\n",
            "epoch 26\n",
            "train_loss 2.15520426745305 train_acc 0.22440856777493606\n",
            "val_loss 2.498517754711682 val_acc 0.09473892405063292\n",
            "epoch 27\n",
            "train_loss 2.1324825866143113 train_acc 0.23523417519181586\n",
            "val_loss 2.5190752095813993 val_acc 0.10057357594936708\n",
            "epoch 28\n",
            "train_loss 2.1111769676208496 train_acc 0.24515265345268542\n",
            "val_loss 2.516118269932421 val_acc 0.09661787974683544\n",
            "epoch 29\n",
            "train_loss 2.0971069186544784 train_acc 0.25310901534526853\n",
            "val_loss 2.534225838093818 val_acc 0.10255142405063292\n",
            "epoch 30\n",
            "train_loss 2.070014160612355 train_acc 0.2648817135549872\n",
            "val_loss 2.589883855626553 val_acc 0.10314477848101265\n",
            "epoch 31\n",
            "train_loss 2.0436373756974553 train_acc 0.2757752557544757\n",
            "val_loss 2.6045706936075717 val_acc 0.10166139240506329\n",
            "epoch 32\n",
            "train_loss 2.0141536216906575 train_acc 0.29112851662404093\n",
            "val_loss 2.6284299862535696 val_acc 0.1027492088607595\n",
            "epoch 33\n",
            "train_loss 2.003673921460691 train_acc 0.29440537084398977\n",
            "val_loss 2.6645422072350224 val_acc 0.10077136075949367\n",
            "epoch 34\n",
            "train_loss 1.9644495596361282 train_acc 0.31017023657289\n",
            "val_loss 2.7116071936450425 val_acc 0.09276107594936708\n",
            "epoch 35\n",
            "train_loss 1.9318117522217733 train_acc 0.32521179667519184\n",
            "val_loss 2.699941372569603 val_acc 0.10571598101265822\n",
            "epoch 36\n",
            "train_loss 1.8996795318315707 train_acc 0.33820332480818416\n",
            "val_loss 2.729169954227496 val_acc 0.10957278481012658\n",
            "epoch 37\n",
            "train_loss 1.8669601096521558 train_acc 0.34936860613810744\n",
            "val_loss 2.751075977011572 val_acc 0.09869462025316456\n",
            "epoch 38\n",
            "train_loss 1.8336625178451733 train_acc 0.3646339514066496\n",
            "val_loss 2.776221583161173 val_acc 0.10452927215189874\n",
            "epoch 39\n",
            "train_loss 1.7930375230891624 train_acc 0.37794517263427113\n",
            "val_loss 2.9657041604005836 val_acc 0.10126582278481013\n",
            "epoch 40\n",
            "train_loss 1.7564814298049263 train_acc 0.39440936700767265\n",
            "val_loss 2.9206773872616925 val_acc 0.10472705696202532\n",
            "epoch 41\n",
            "train_loss 1.7227690067437604 train_acc 0.4060022378516624\n",
            "val_loss 2.8940889231766325 val_acc 0.0992879746835443\n",
            "epoch 42\n",
            "train_loss 1.6710928876686584 train_acc 0.42535166240409206\n",
            "val_loss 2.9946596592287476 val_acc 0.10166139240506329\n",
            "epoch 43\n",
            "train_loss 1.6447607211749573 train_acc 0.43390744884910487\n",
            "val_loss 3.002269630190692 val_acc 0.10452927215189874\n",
            "epoch 44\n",
            "train_loss 1.6030081781889776 train_acc 0.45128676470588236\n",
            "val_loss 3.036990736104265 val_acc 0.10304588607594936\n",
            "epoch 45\n",
            "train_loss 1.556011842949616 train_acc 0.46608855498721224\n",
            "val_loss 3.0973258682444125 val_acc 0.10116693037974683\n",
            "epoch 46\n",
            "train_loss 1.5213368109729895 train_acc 0.47811700767263426\n",
            "val_loss 3.184372790252106 val_acc 0.09879351265822785\n",
            "epoch 47\n",
            "train_loss 1.4947052694037748 train_acc 0.4906449808184143\n",
            "val_loss 3.2249406440348567 val_acc 0.10630933544303797\n",
            "epoch 48\n",
            "train_loss 1.4410164438550124 train_acc 0.5074408567774936\n",
            "val_loss 3.274600620511212 val_acc 0.10690268987341772\n",
            "epoch 49\n",
            "train_loss 1.4084367938053883 train_acc 0.520108695652174\n",
            "val_loss 3.435001162034047 val_acc 0.09632120253164557\n",
            "epoch 50\n",
            "train_loss 1.3676503276276162 train_acc 0.5334918478260869\n",
            "val_loss 3.4027120916149283 val_acc 0.10126582278481013\n",
            "epoch 51\n",
            "train_loss 1.3259481391333559 train_acc 0.5485334079283888\n",
            "val_loss 3.5061504961569097 val_acc 0.09553006329113924\n",
            "epoch 52\n",
            "train_loss 1.2978231879451392 train_acc 0.5573009910485933\n",
            "val_loss 3.4491699013528945 val_acc 0.10581487341772151\n",
            "epoch 53\n",
            "train_loss 1.2539375791769198 train_acc 0.5716152493606138\n",
            "val_loss 3.5201816287221788 val_acc 0.10571598101265822\n",
            "epoch 54\n",
            "train_loss 1.2315207285344447 train_acc 0.5810661764705882\n",
            "val_loss 3.591354333901707 val_acc 0.09503560126582279\n",
            "epoch 55\n",
            "train_loss 1.1954500112692108 train_acc 0.5943294437340153\n",
            "val_loss 3.7426440051839323 val_acc 0.0925632911392405\n",
            "epoch 56\n",
            "train_loss 1.1684438170069624 train_acc 0.6034606777493606\n",
            "val_loss 3.657256201852726 val_acc 0.10334256329113924\n",
            "epoch 57\n",
            "train_loss 1.1018069465751843 train_acc 0.6272618286445013\n",
            "val_loss 3.882111781760107 val_acc 0.09849683544303797\n",
            "epoch 58\n",
            "train_loss 1.0730847667550187 train_acc 0.6369045716112531\n",
            "val_loss 3.923933515065833 val_acc 0.10067246835443038\n",
            "epoch 59\n",
            "train_loss 1.0508370025993308 train_acc 0.6435741687979539\n",
            "val_loss 3.9627790541588506 val_acc 0.10452927215189874\n",
            "epoch 60\n",
            "train_loss 1.0317860846324345 train_acc 0.6513307225063939\n",
            "val_loss 3.982562729074985 val_acc 0.09444224683544304\n",
            "epoch 61\n",
            "train_loss 0.9866438369311945 train_acc 0.6640664961636829\n",
            "val_loss 4.0397636196281335 val_acc 0.1003757911392405\n",
            "epoch 62\n",
            "train_loss 0.9536358988498483 train_acc 0.6774696291560103\n",
            "val_loss 4.011833525911162 val_acc 0.10799050632911393\n",
            "epoch 63\n",
            "train_loss 0.9329581146349992 train_acc 0.6851982097186702\n",
            "val_loss 4.069844707658019 val_acc 0.10344145569620253\n",
            "epoch 64\n",
            "train_loss 0.9072410638070167 train_acc 0.6915960677749361\n",
            "val_loss 4.260118888903268 val_acc 0.09651898734177215\n",
            "epoch 65\n",
            "train_loss 0.8859640297377506 train_acc 0.6985374040920717\n",
            "val_loss 4.22550628758684 val_acc 0.10334256329113924\n",
            "epoch 66\n",
            "train_loss 0.8561454063181377 train_acc 0.7098545396419438\n",
            "val_loss 4.2934387877017635 val_acc 0.10245253164556962\n",
            "epoch 67\n",
            "train_loss 0.8281262494109171 train_acc 0.7210877557544757\n",
            "val_loss 4.374747783322878 val_acc 0.10096914556962025\n",
            "epoch 68\n",
            "train_loss 0.8108368677556362 train_acc 0.7266783887468031\n",
            "val_loss 4.416286580170257 val_acc 0.0972112341772152\n",
            "epoch 69\n",
            "train_loss 0.7817962873927162 train_acc 0.7357816496163683\n",
            "val_loss 4.51667241205143 val_acc 0.10057357594936708\n",
            "epoch 70\n",
            "train_loss 0.7656686908143866 train_acc 0.7422554347826087\n",
            "val_loss 4.573489593554147 val_acc 0.09740901898734178\n",
            "epoch 71\n",
            "train_loss 0.7451125336120196 train_acc 0.7501958120204604\n",
            "val_loss 4.530809933626199 val_acc 0.1037381329113924\n",
            "epoch 72\n",
            "train_loss 0.7320668916873005 train_acc 0.7537004475703324\n",
            "val_loss 4.469787772697739 val_acc 0.10205696202531646\n",
            "epoch 73\n",
            "train_loss 0.6888690290548612 train_acc 0.7680306905370844\n",
            "val_loss 4.5924626966066 val_acc 0.09879351265822785\n",
            "epoch 74\n",
            "train_loss 0.6674084927877197 train_acc 0.776474584398977\n",
            "val_loss 4.727110231978984 val_acc 0.1050237341772152\n",
            "epoch 75\n",
            "train_loss 0.6506040004055823 train_acc 0.780246962915601\n",
            "val_loss 4.671630328214621 val_acc 0.1059137658227848\n",
            "epoch 76\n",
            "train_loss 0.6222428190891091 train_acc 0.7900015984654731\n",
            "val_loss 4.954596730727184 val_acc 0.09790348101265822\n",
            "epoch 77\n",
            "train_loss 0.6160762659881426 train_acc 0.7928148976982098\n",
            "val_loss 4.966552909416489 val_acc 0.09058544303797468\n",
            "epoch 78\n",
            "train_loss 0.591672493826093 train_acc 0.8009151214833758\n",
            "val_loss 5.106513578680497 val_acc 0.09879351265822785\n",
            "epoch 79\n",
            "train_loss 0.5878381633087802 train_acc 0.8030450767263427\n",
            "val_loss 5.0772075049484835 val_acc 0.10611155063291139\n",
            "epoch 80\n",
            "train_loss 0.541051633370197 train_acc 0.8173273657289002\n",
            "val_loss 5.23365200018581 val_acc 0.10047468354430379\n",
            "epoch 81\n",
            "train_loss 0.5621781014572934 train_acc 0.8117567135549872\n",
            "val_loss 5.0990585435794875 val_acc 0.10265031645569621\n",
            "epoch 82\n",
            "train_loss 0.5416203786040206 train_acc 0.8175751278772379\n",
            "val_loss 5.145828210854832 val_acc 0.10334256329113924\n",
            "epoch 83\n",
            "train_loss 0.5154110540057082 train_acc 0.8266983695652174\n",
            "val_loss 5.245391815523558 val_acc 0.10096914556962025\n",
            "epoch 84\n",
            "train_loss 0.5205809032673117 train_acc 0.8267703005115089\n",
            "val_loss 5.194260905060587 val_acc 0.10799050632911393\n",
            "epoch 85\n",
            "train_loss 0.49086775826981 train_acc 0.8358495843989769\n",
            "val_loss 5.332736069643045 val_acc 0.09740901898734178\n",
            "epoch 86\n",
            "train_loss 0.4812508906092485 train_acc 0.8371483375959079\n",
            "val_loss 5.47215918649601 val_acc 0.09444224683544304\n",
            "epoch 87\n",
            "train_loss 0.45883275290279435 train_acc 0.8447090792838875\n",
            "val_loss 5.47346663173241 val_acc 0.09978243670886076\n",
            "epoch 88\n",
            "train_loss 0.45300180264903456 train_acc 0.8492287404092072\n",
            "val_loss 5.446338056009027 val_acc 0.1015625\n",
            "epoch 89\n",
            "train_loss 0.44471059705290344 train_acc 0.8523697250639386\n",
            "val_loss 5.446819051911559 val_acc 0.10245253164556962\n",
            "epoch 90\n",
            "train_loss 0.44716148642475345 train_acc 0.8523417519181585\n",
            "val_loss 5.460658248466782 val_acc 0.09839794303797468\n",
            "epoch 91\n",
            "train_loss 0.40712123690053936 train_acc 0.8640385230179028\n",
            "val_loss 5.402941697760474 val_acc 0.11244066455696203\n",
            "epoch 92\n",
            "train_loss 0.4028522937803927 train_acc 0.8633112212276215\n",
            "val_loss 5.67646540267558 val_acc 0.09998022151898735\n",
            "epoch 93\n",
            "train_loss 0.4087506899672091 train_acc 0.8629915281329923\n",
            "val_loss 5.487187047547932 val_acc 0.10087025316455696\n",
            "epoch 94\n",
            "train_loss 0.3886072978644115 train_acc 0.8709798593350384\n",
            "val_loss 5.7052326805983915 val_acc 0.0982001582278481\n",
            "epoch 95\n",
            "train_loss 0.38202559433477307 train_acc 0.8730778452685423\n",
            "val_loss 5.747805969624579 val_acc 0.10106803797468354\n",
            "epoch 96\n",
            "train_loss 0.3720755635777398 train_acc 0.8767822890025575\n",
            "val_loss 5.716795034046415 val_acc 0.10551819620253164\n",
            "epoch 97\n",
            "train_loss 0.35400880252003974 train_acc 0.8829164002557545\n",
            "val_loss 5.831266173833533 val_acc 0.0972112341772152\n",
            "epoch 98\n",
            "train_loss 0.3448036389658823 train_acc 0.8868765984654732\n",
            "val_loss 5.790100797822204 val_acc 0.10522151898734178\n",
            "epoch 99\n",
            "train_loss 0.3550595127217605 train_acc 0.8821771099744244\n",
            "val_loss 5.817454579510266 val_acc 0.09701344936708861\n",
            "epoch 100\n",
            "train_loss 0.32621241200839163 train_acc 0.892067615089514\n",
            "val_loss 5.921127192581756 val_acc 0.10126582278481013\n",
            "epoch 101\n",
            "train_loss 0.3393247164881138 train_acc 0.8875799232736573\n",
            "val_loss 5.960345853733111 val_acc 0.10541930379746836\n",
            "epoch 102\n",
            "train_loss 0.3400083655286628 train_acc 0.8879955242966753\n",
            "val_loss 5.775007429002207 val_acc 0.10472705696202532\n",
            "epoch 103\n",
            "train_loss 0.3173201842914762 train_acc 0.8936980498721229\n",
            "val_loss 6.0397658287724365 val_acc 0.09355221518987342\n",
            "epoch 104\n",
            "train_loss 0.3188841129126756 train_acc 0.8946131713554987\n",
            "val_loss 6.0205092369755615 val_acc 0.09909018987341772\n",
            "epoch 105\n",
            "train_loss 0.3105578742483083 train_acc 0.8953844309462915\n",
            "val_loss 6.060686654682401 val_acc 0.09849683544303797\n",
            "epoch 106\n",
            "train_loss 0.2954908939045104 train_acc 0.9014426150895141\n",
            "val_loss 6.1756342030778715 val_acc 0.09701344936708861\n",
            "epoch 107\n",
            "train_loss 0.3021726394286546 train_acc 0.9009590792838874\n",
            "val_loss 6.101914592936069 val_acc 0.09454113924050633\n",
            "epoch 108\n",
            "train_loss 0.28307634963632544 train_acc 0.9059702685421995\n",
            "val_loss 6.084906324555602 val_acc 0.10057357594936708\n",
            "epoch 109\n",
            "train_loss 0.280573852893794 train_acc 0.9087755754475704\n",
            "val_loss 5.886877403983587 val_acc 0.1071004746835443\n",
            "epoch 110\n",
            "train_loss 0.2703636976344811 train_acc 0.9109414961636829\n",
            "val_loss 6.150978353959095 val_acc 0.10017800632911393\n",
            "epoch 111\n",
            "train_loss 0.26718339539321184 train_acc 0.9131993286445014\n",
            "val_loss 6.2650369632093215 val_acc 0.0971123417721519\n",
            "epoch 112\n",
            "train_loss 0.25897112342021655 train_acc 0.9145260549872122\n",
            "val_loss 6.428118947186047 val_acc 0.09562895569620253\n",
            "epoch 113\n",
            "train_loss 0.28562662542780953 train_acc 0.9057944373401534\n",
            "val_loss 6.10497234441057 val_acc 0.0993868670886076\n",
            "epoch 114\n",
            "train_loss 0.2607995542838141 train_acc 0.9144021739130435\n",
            "val_loss 6.266836667362647 val_acc 0.10532041139240507\n",
            "epoch 115\n",
            "train_loss 0.2502548090370415 train_acc 0.9168478260869566\n",
            "val_loss 6.244085263602341 val_acc 0.1081882911392405\n",
            "epoch 116\n",
            "train_loss 0.24947095279346038 train_acc 0.9179907289002557\n",
            "val_loss 6.314665522756456 val_acc 0.09464003164556962\n",
            "epoch 117\n",
            "train_loss 0.23000350640253034 train_acc 0.9237092391304348\n",
            "val_loss 6.48163657852366 val_acc 0.10660601265822785\n",
            "epoch 118\n",
            "train_loss 0.23513465254660457 train_acc 0.9215632992327366\n",
            "val_loss 6.561857181259349 val_acc 0.09790348101265822\n",
            "epoch 119\n",
            "train_loss 0.2524252646909955 train_acc 0.9168957800511509\n",
            "val_loss 6.469544676285755 val_acc 0.09731012658227849\n",
            "epoch 120\n",
            "train_loss 0.23127467824560602 train_acc 0.9240129475703325\n",
            "val_loss 6.49465468563611 val_acc 0.09839794303797468\n",
            "epoch 121\n",
            "train_loss 0.2261110844895663 train_acc 0.9256034207161125\n",
            "val_loss 6.5610593964781945 val_acc 0.10067246835443038\n",
            "epoch 122\n",
            "train_loss 0.21352500665713758 train_acc 0.9301430626598466\n",
            "val_loss 6.650420864926109 val_acc 0.0972112341772152\n",
            "epoch 123\n",
            "train_loss 0.22156585177497182 train_acc 0.9271539322250639\n",
            "val_loss 6.589169496222388 val_acc 0.09859572784810126\n",
            "epoch 124\n",
            "train_loss 0.20899182369413277 train_acc 0.9315097506393861\n",
            "val_loss 6.6299165894713585 val_acc 0.09889240506329114\n",
            "epoch 125\n",
            "train_loss 0.22077050960391684 train_acc 0.9274056905370844\n",
            "val_loss 6.457160490977613 val_acc 0.10413370253164557\n",
            "epoch 126\n",
            "train_loss 0.19782772713609972 train_acc 0.9349624360613811\n",
            "val_loss 6.45601708375955 val_acc 0.1015625\n",
            "epoch 127\n",
            "train_loss 0.20562475329012517 train_acc 0.9319493286445012\n",
            "val_loss 6.66224421126933 val_acc 0.09829905063291139\n",
            "epoch 128\n",
            "train_loss 0.19612681588439076 train_acc 0.935465952685422\n",
            "val_loss 6.621834579902359 val_acc 0.10354034810126582\n",
            "epoch 129\n",
            "train_loss 0.1927350850208946 train_acc 0.9373841112531969\n",
            "val_loss 6.566352319113816 val_acc 0.1050237341772152\n",
            "epoch 130\n",
            "train_loss 0.20561073486076292 train_acc 0.9325487531969309\n",
            "val_loss 6.743009621583963 val_acc 0.09701344936708861\n",
            "epoch 131\n",
            "train_loss 0.1924954836097215 train_acc 0.9372882033248082\n",
            "val_loss 6.688025033926662 val_acc 0.0992879746835443\n",
            "epoch 132\n",
            "train_loss 0.20043095910107084 train_acc 0.9342790920716113\n",
            "val_loss 6.718789106682886 val_acc 0.10255142405063292\n",
            "epoch 133\n",
            "train_loss 0.17426437839789463 train_acc 0.9425671355498721\n",
            "val_loss 6.862906099874762 val_acc 0.09998022151898735\n",
            "epoch 134\n",
            "train_loss 0.18679316249459296 train_acc 0.939298273657289\n",
            "val_loss 6.882527375523048 val_acc 0.09701344936708861\n",
            "epoch 135\n",
            "train_loss 0.1765468020821014 train_acc 0.9412164322250639\n",
            "val_loss 6.861136370067355 val_acc 0.09869462025316456\n",
            "epoch 136\n",
            "train_loss 0.18066972872370954 train_acc 0.9409247122762149\n",
            "val_loss 6.808139034464389 val_acc 0.0993868670886076\n",
            "epoch 137\n",
            "train_loss 0.17708111421950637 train_acc 0.9428668478260869\n",
            "val_loss 6.889393945283528 val_acc 0.10265031645569621\n",
            "epoch 138\n",
            "train_loss 0.1716872394237372 train_acc 0.9436061381074169\n",
            "val_loss 6.670513503159149 val_acc 0.10482594936708861\n",
            "epoch 139\n",
            "train_loss 0.17422147164278476 train_acc 0.9428828324808184\n",
            "val_loss 6.999634501300281 val_acc 0.09849683544303797\n",
            "epoch 140\n",
            "train_loss 0.15786873836479987 train_acc 0.9481297953964194\n",
            "val_loss 7.2120851142497004 val_acc 0.10096914556962025\n",
            "epoch 141\n",
            "train_loss 0.17254189613377652 train_acc 0.9430187020460358\n",
            "val_loss 7.048560468456413 val_acc 0.0960245253164557\n",
            "epoch 142\n",
            "train_loss 0.15762566359680327 train_acc 0.948641304347826\n",
            "val_loss 6.993959819214253 val_acc 0.10106803797468354\n",
            "epoch 143\n",
            "train_loss 0.1617992324254397 train_acc 0.9480978260869566\n",
            "val_loss 6.847081286997735 val_acc 0.10007911392405064\n",
            "epoch 144\n",
            "train_loss 0.14763986473650578 train_acc 0.9519301470588235\n",
            "val_loss 6.99461553670183 val_acc 0.1038370253164557\n",
            "epoch 145\n",
            "train_loss 0.16501039944951187 train_acc 0.944960837595908\n",
            "val_loss 7.037757601919053 val_acc 0.0972112341772152\n",
            "epoch 146\n",
            "train_loss 0.14921360748731877 train_acc 0.9499560421994885\n",
            "val_loss 7.095538911940176 val_acc 0.09444224683544304\n",
            "epoch 147\n",
            "train_loss 0.14164560851271804 train_acc 0.95392023657289\n",
            "val_loss 7.013199643243717 val_acc 0.10077136075949367\n",
            "epoch 148\n",
            "train_loss 0.14504051716793376 train_acc 0.952361732736573\n",
            "val_loss 7.188362767424764 val_acc 0.10195806962025317\n",
            "epoch 149\n",
            "train_loss 0.14845379101364967 train_acc 0.9518342391304349\n",
            "val_loss 7.100799294966686 val_acc 0.09859572784810126\n",
            "epoch 150\n",
            "train_loss 0.14698731638205326 train_acc 0.9525455562659847\n",
            "val_loss 7.054692618454559 val_acc 0.10146360759493671\n",
            "epoch 151\n",
            "train_loss 0.14195295104094782 train_acc 0.9535246163682864\n",
            "val_loss 7.061812654326234 val_acc 0.10215585443037975\n",
            "epoch 152\n",
            "train_loss 0.1412198923890243 train_acc 0.9541799872122763\n",
            "val_loss 7.180278892758526 val_acc 0.1059137658227848\n",
            "epoch 153\n",
            "train_loss 0.1360258085038656 train_acc 0.9564418158567775\n",
            "val_loss 7.256648099875148 val_acc 0.10136471518987342\n",
            "epoch 154\n",
            "train_loss 0.13980153953785177 train_acc 0.9544677109974424\n",
            "val_loss 7.2001274446897865 val_acc 0.10017800632911393\n",
            "epoch 155\n",
            "train_loss 0.1348974806544802 train_acc 0.9556066176470588\n",
            "val_loss 7.37202198897736 val_acc 0.09187104430379747\n",
            "epoch 156\n",
            "train_loss 0.11961391608676185 train_acc 0.9602461636828645\n",
            "val_loss 7.408484374420552 val_acc 0.0981012658227848\n",
            "epoch 157\n",
            "train_loss 0.12904104757148896 train_acc 0.9577685421994885\n",
            "val_loss 7.335686267176761 val_acc 0.09681566455696203\n",
            "epoch 158\n",
            "train_loss 0.11636245194012704 train_acc 0.9609534846547315\n",
            "val_loss 7.3848409109477755 val_acc 0.09889240506329114\n",
            "epoch 159\n",
            "train_loss 0.1411414294863296 train_acc 0.9547394501278773\n",
            "val_loss 7.306153273280663 val_acc 0.10314477848101265\n",
            "epoch 160\n",
            "train_loss 0.12567888524221338 train_acc 0.9586836636828644\n",
            "val_loss 7.259865096852749 val_acc 0.09998022151898735\n",
            "epoch 161\n",
            "train_loss 0.11399077089584392 train_acc 0.9620883951406649\n",
            "val_loss 7.543603305575214 val_acc 0.09483781645569621\n",
            "epoch 162\n",
            "train_loss 0.1330855903037064 train_acc 0.9565257352941176\n",
            "val_loss 7.194037721126894 val_acc 0.10581487341772151\n",
            "epoch 163\n",
            "train_loss 0.11221377585854982 train_acc 0.9638387148337595\n",
            "val_loss 7.394058215467235 val_acc 0.08949762658227849\n",
            "epoch 164\n",
            "train_loss 0.10821395240905111 train_acc 0.9650295716112532\n",
            "val_loss 7.539352121232431 val_acc 0.09404667721518987\n",
            "epoch 165\n",
            "train_loss 0.11775088265938374 train_acc 0.9617806905370844\n",
            "val_loss 7.567863886869406 val_acc 0.09444224683544304\n",
            "epoch 166\n",
            "train_loss 0.12323799665035952 train_acc 0.960230179028133\n",
            "val_loss 7.51069681553901 val_acc 0.09790348101265822\n",
            "epoch 167\n",
            "train_loss 0.11107904047650449 train_acc 0.9634031329923274\n",
            "val_loss 7.389924683148348 val_acc 0.10294699367088607\n",
            "epoch 168\n",
            "train_loss 0.109597373401265 train_acc 0.9639865728900255\n",
            "val_loss 7.571695798560034 val_acc 0.09642009493670886\n",
            "epoch 169\n",
            "train_loss 0.1249709032580752 train_acc 0.9592511189258311\n",
            "val_loss 7.405438755131975 val_acc 0.10215585443037975\n",
            "epoch 170\n",
            "train_loss 0.11514988501110802 train_acc 0.9623321611253197\n",
            "val_loss 7.391752617268622 val_acc 0.1050237341772152\n",
            "Epoch   171: reducing learning rate of group 0 to 1.0000e-03.\n",
            "epoch 171\n",
            "train_loss 0.08230300193838298 train_acc 0.9736373081841432\n",
            "val_loss 7.375807641427728 val_acc 0.09701344936708861\n",
            "epoch 172\n",
            "train_loss 0.06537297183928815 train_acc 0.9794677109974425\n",
            "val_loss 7.234234061422227 val_acc 0.09968354430379747\n",
            "epoch 173\n",
            "train_loss 0.058885267359273664 train_acc 0.9811700767263427\n",
            "val_loss 7.452809339837183 val_acc 0.10136471518987342\n",
            "epoch 174\n",
            "train_loss 0.05075725374857674 train_acc 0.9842950767263428\n",
            "val_loss 7.31156489818911 val_acc 0.10017800632911393\n",
            "epoch 175\n",
            "train_loss 0.044863636584004475 train_acc 0.9861492966751919\n",
            "val_loss 7.336108467246913 val_acc 0.10255142405063292\n",
            "epoch 176\n",
            "train_loss 0.043931719359925105 train_acc 0.9867846867007672\n",
            "val_loss 7.323729949661448 val_acc 0.10215585443037975\n",
            "epoch 177\n",
            "train_loss 0.04087231484631224 train_acc 0.9878476662404093\n",
            "val_loss 7.4197533885134925 val_acc 0.10294699367088607\n",
            "epoch 178\n",
            "train_loss 0.04045786537096628 train_acc 0.9878716432225064\n",
            "val_loss 7.436065927336488 val_acc 0.09800237341772151\n",
            "epoch 179\n",
            "train_loss 0.03904544722284083 train_acc 0.9882472826086958\n",
            "val_loss 7.375920784624317 val_acc 0.09998022151898735\n",
            "epoch 180\n",
            "train_loss 0.03701741252061161 train_acc 0.988678868286445\n",
            "val_loss 7.4894065917292725 val_acc 0.10087025316455696\n",
            "epoch 181\n",
            "train_loss 0.03523680881735252 train_acc 0.9892503196930946\n",
            "val_loss 7.427018044870111 val_acc 0.10116693037974683\n",
            "epoch 182\n",
            "train_loss 0.0322236195611565 train_acc 0.9900855179028134\n",
            "val_loss 7.557272554952887 val_acc 0.10116693037974683\n",
            "epoch 183\n",
            "train_loss 0.03325554915397759 train_acc 0.9901894181585678\n",
            "val_loss 7.564920509917827 val_acc 0.10403481012658228\n",
            "epoch 184\n",
            "train_loss 0.03592903794788653 train_acc 0.989358216112532\n",
            "val_loss 7.665215027483204 val_acc 0.10265031645569621\n",
            "epoch 185\n",
            "train_loss 0.03743029004879905 train_acc 0.9891104539641944\n",
            "val_loss 7.527185476278957 val_acc 0.0992879746835443\n",
            "epoch 186\n",
            "train_loss 0.031190489098439207 train_acc 0.990417199488491\n",
            "val_loss 7.619763827022118 val_acc 0.10195806962025317\n",
            "epoch 187\n",
            "train_loss 0.028438184455589717 train_acc 0.9912364130434783\n",
            "val_loss 7.602218458924113 val_acc 0.1015625\n",
            "epoch 188\n",
            "train_loss 0.03250332878392828 train_acc 0.9901974104859336\n",
            "val_loss 7.628227789190751 val_acc 0.0993868670886076\n",
            "epoch 189\n",
            "train_loss 0.02905286771639505 train_acc 0.9911484974424553\n",
            "val_loss 7.574311703066282 val_acc 0.1037381329113924\n",
            "epoch 190\n",
            "train_loss 0.028334451284643042 train_acc 0.9916360294117648\n",
            "val_loss 7.608925372739382 val_acc 0.10284810126582279\n",
            "epoch 191\n",
            "train_loss 0.028914517992178496 train_acc 0.9912044437340154\n",
            "val_loss 7.600617288034173 val_acc 0.10255142405063292\n",
            "epoch 192\n",
            "train_loss 0.02716001639674987 train_acc 0.9919357416879796\n",
            "val_loss 7.648063297513165 val_acc 0.10304588607594936\n",
            "epoch 193\n",
            "train_loss 0.02994003602693005 train_acc 0.9905770460358057\n",
            "val_loss 7.734865937051894 val_acc 0.10284810126582279\n",
            "epoch 194\n",
            "train_loss 0.02757279781992083 train_acc 0.9914721867007672\n",
            "val_loss 7.820440129388737 val_acc 0.10007911392405064\n",
            "epoch 195\n",
            "train_loss 0.030182120408100623 train_acc 0.9913363171355499\n",
            "val_loss 7.836553905583635 val_acc 0.1015625\n",
            "epoch 196\n",
            "train_loss 0.02418915129652547 train_acc 0.9930666560102301\n",
            "val_loss 7.7983046060876005 val_acc 0.10007911392405064\n",
            "epoch 197\n",
            "train_loss 0.026391288793176565 train_acc 0.9923673273657289\n",
            "val_loss 7.8013301559641395 val_acc 0.10116693037974683\n",
            "epoch 198\n",
            "train_loss 0.027920637674697574 train_acc 0.9918598145780051\n",
            "val_loss 7.779810965815677 val_acc 0.09632120253164557\n",
            "epoch 199\n",
            "train_loss 0.025723182948787348 train_acc 0.991895780051151\n",
            "val_loss 7.889022265808491 val_acc 0.09968354430379747\n",
            "epoch 200\n",
            "train_loss 0.022365347545533237 train_acc 0.993394341432225\n",
            "val_loss 7.8806713623336595 val_acc 0.09790348101265822\n",
            "epoch 201\n",
            "train_loss 0.025336038431419475 train_acc 0.9921155690537085\n",
            "val_loss 7.900593612767473 val_acc 0.09770569620253164\n",
            "epoch 202\n",
            "train_loss 0.022395950420956125 train_acc 0.9934702685421994\n",
            "val_loss 7.826806841017325 val_acc 0.1071993670886076\n",
            "epoch 203\n",
            "train_loss 0.025318095869625277 train_acc 0.9920276534526854\n",
            "val_loss 7.99164988119391 val_acc 0.09681566455696203\n",
            "epoch 204\n",
            "train_loss 0.022828695206793353 train_acc 0.993486253196931\n",
            "val_loss 7.929516556896741 val_acc 0.09740901898734178\n",
            "epoch 205\n",
            "train_loss 0.02261754158301317 train_acc 0.9932145140664962\n",
            "val_loss 7.95128082323678 val_acc 0.1003757911392405\n",
            "epoch 206\n",
            "train_loss 0.023057824612119953 train_acc 0.9931865409207161\n",
            "val_loss 7.930555838572828 val_acc 0.09918908227848101\n",
            "Epoch   207: reducing learning rate of group 0 to 1.0000e-04.\n",
            "epoch 207\n",
            "train_loss 0.024121229955747895 train_acc 0.992307384910486\n",
            "val_loss 7.948751938493946 val_acc 0.10284810126582279\n",
            "epoch 208\n",
            "train_loss 0.02332761317022297 train_acc 0.9932864450127877\n",
            "val_loss 7.972164226483695 val_acc 0.09859572784810126\n",
            "epoch 209\n",
            "train_loss 0.021048239549286092 train_acc 0.9940057544757033\n",
            "val_loss 7.972361999221995 val_acc 0.1038370253164557\n",
            "epoch 210\n",
            "train_loss 0.024218660370652775 train_acc 0.9930226982097188\n",
            "val_loss 7.840979413141178 val_acc 0.10027689873417721\n",
            "epoch 211\n",
            "train_loss 0.02103013884352968 train_acc 0.9936141304347826\n",
            "val_loss 8.00309312796291 val_acc 0.09988132911392406\n",
            "epoch 212\n",
            "train_loss 0.018981608724884948 train_acc 0.9942934782608696\n",
            "val_loss 7.940971652163735 val_acc 0.09948575949367089\n",
            "epoch 213\n",
            "train_loss 0.024129175185345 train_acc 0.9926230818414323\n",
            "val_loss 7.9700751304626465 val_acc 0.09909018987341772\n",
            "epoch 214\n",
            "train_loss 0.02287298877773535 train_acc 0.9930506713554986\n",
            "val_loss 7.950294385982465 val_acc 0.09968354430379747\n",
            "epoch 215\n",
            "train_loss 0.021734971202203714 train_acc 0.9939138427109975\n",
            "val_loss 7.943727475178393 val_acc 0.10017800632911393\n",
            "epoch 216\n",
            "train_loss 0.022385550348613354 train_acc 0.9931665601023018\n",
            "val_loss 7.942265745959705 val_acc 0.10294699367088607\n",
            "epoch 217\n",
            "train_loss 0.01952257262968013 train_acc 0.9942535166240409\n",
            "val_loss 7.875728100161009 val_acc 0.10363924050632911\n",
            "epoch 218\n",
            "train_loss 0.0195642977667427 train_acc 0.9944932864450128\n",
            "val_loss 7.934837256805806 val_acc 0.10294699367088607\n",
            "epoch 219\n",
            "train_loss 0.0244543844788714 train_acc 0.9927629475703326\n",
            "val_loss 8.0199614476554 val_acc 0.10452927215189874\n",
            "epoch 220\n",
            "train_loss 0.02031952252282816 train_acc 0.9937819693094629\n",
            "val_loss 8.007823998415017 val_acc 0.09899129746835443\n",
            "epoch 221\n",
            "train_loss 0.02067900253498517 train_acc 0.9939617966751919\n",
            "val_loss 8.059400244604182 val_acc 0.10087025316455696\n",
            "epoch 222\n",
            "train_loss 0.018487301503565364 train_acc 0.9943014705882354\n",
            "val_loss 7.983288668378999 val_acc 0.09998022151898735\n",
            "epoch 223\n",
            "train_loss 0.02097870737178456 train_acc 0.9938539002557545\n",
            "val_loss 7.944121463389337 val_acc 0.09889240506329114\n",
            "epoch 224\n",
            "train_loss 0.020475762690652086 train_acc 0.9939817774936062\n",
            "val_loss 8.001568238946456 val_acc 0.09948575949367089\n",
            "Epoch   225: reducing learning rate of group 0 to 1.0000e-05.\n",
            "epoch 225\n",
            "train_loss 0.022241059449695462 train_acc 0.9932944373401534\n",
            "val_loss 7.970730829842483 val_acc 0.10096914556962025\n",
            "epoch 226\n",
            "train_loss 0.021558252793988045 train_acc 0.9932544757033248\n",
            "val_loss 8.031181812286377 val_acc 0.0960245253164557\n",
            "epoch 227\n",
            "train_loss 0.020432846724142646 train_acc 0.993925831202046\n",
            "val_loss 7.85424217996718 val_acc 0.10087025316455696\n",
            "epoch 228\n",
            "train_loss 0.020475790945335488 train_acc 0.994045716112532\n",
            "val_loss 7.9163183743440655 val_acc 0.10166139240506329\n",
            "epoch 229\n",
            "train_loss 0.02126297023874717 train_acc 0.9935062340153452\n",
            "val_loss 8.062736873385273 val_acc 0.09829905063291139\n",
            "epoch 230\n",
            "train_loss 0.01862329588113281 train_acc 0.9945532289002558\n",
            "val_loss 8.020998375325263 val_acc 0.10106803797468354\n",
            "Epoch   231: reducing learning rate of group 0 to 1.0000e-06.\n",
            "epoch 231\n",
            "train_loss 0.020679811251458718 train_acc 0.9937260230179028\n",
            "val_loss 8.070359598232221 val_acc 0.09750791139240507\n",
            "epoch 232\n",
            "train_loss 0.017845940180843376 train_acc 0.9948249680306905\n",
            "val_loss 8.04598399053646 val_acc 0.09869462025316456\n",
            "epoch 233\n",
            "train_loss 0.019976324541434703 train_acc 0.9945132672634271\n",
            "val_loss 7.858428695533849 val_acc 0.10225474683544304\n",
            "epoch 234\n",
            "train_loss 0.017973797761298038 train_acc 0.9948249680306905\n",
            "val_loss 7.888552255268339 val_acc 0.09770569620253164\n",
            "epoch 235\n",
            "train_loss 0.02290700462154325 train_acc 0.9933703644501278\n",
            "val_loss 7.903538064111637 val_acc 0.09918908227848101\n",
            "epoch 236\n",
            "train_loss 0.018351573406544794 train_acc 0.994693094629156\n",
            "val_loss 7.944753574419625 val_acc 0.10067246835443038\n",
            "epoch 237\n",
            "train_loss 0.02017899201212265 train_acc 0.9936900575447569\n",
            "val_loss 8.01657411116588 val_acc 0.1037381329113924\n",
            "epoch 238\n",
            "train_loss 0.024771584122849015 train_acc 0.9924632352941177\n",
            "val_loss 8.043914969963364 val_acc 0.1015625\n",
            "Epoch   239: reducing learning rate of group 0 to 1.0000e-07.\n",
            "epoch 239\n",
            "train_loss 0.020981746500648577 train_acc 0.9936261189258312\n",
            "val_loss 7.921054840087891 val_acc 0.10087025316455696\n",
            "epoch 240\n",
            "train_loss 0.02199814310992587 train_acc 0.993634111253197\n",
            "val_loss 7.957287015794199 val_acc 0.09909018987341772\n",
            "epoch 241\n",
            "train_loss 0.02002026539479318 train_acc 0.9939138427109975\n",
            "val_loss 7.981035564519182 val_acc 0.10225474683544304\n",
            "epoch 242\n",
            "train_loss 0.021210797243904025 train_acc 0.9942255434782609\n",
            "val_loss 8.058398464058019 val_acc 0.09948575949367089\n",
            "epoch 243\n",
            "train_loss 0.0205253599373245 train_acc 0.9940057544757033\n",
            "val_loss 8.055683099770848 val_acc 0.10136471518987342\n",
            "epoch 244\n",
            "train_loss 0.018629833891847268 train_acc 0.994605179028133\n",
            "val_loss 7.992037936101986 val_acc 0.0992879746835443\n",
            "Epoch   245: reducing learning rate of group 0 to 1.0000e-08.\n",
            "epoch 245\n",
            "train_loss 0.022832732563993187 train_acc 0.9932944373401534\n",
            "val_loss 7.875623202022118 val_acc 0.10077136075949367\n",
            "epoch 246\n",
            "train_loss 0.020138257696433827 train_acc 0.9937739769820972\n",
            "val_loss 7.991937124276463 val_acc 0.10027689873417721\n",
            "epoch 247\n",
            "train_loss 0.024814162145355655 train_acc 0.9926950127877238\n",
            "val_loss 7.954364927509163 val_acc 0.10284810126582279\n",
            "epoch 248\n",
            "train_loss 0.024782305757683056 train_acc 0.9929747442455243\n",
            "val_loss 7.935894302175015 val_acc 0.10057357594936708\n",
            "epoch 249\n",
            "train_loss 0.022281744926700445 train_acc 0.9933064258312021\n",
            "val_loss 7.947691386259055 val_acc 0.1015625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rdMoxyzgeo2k",
        "outputId": "28c588da-0cd1-416d-b933-159cf005d872"
      },
      "source": [
        "plt.plot(losses_old)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5e49d692d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe6klEQVR4nO3deZhU1Z3/8fe3tt73DegGGgQRFFRoEZckLjE/NRoTNXF7AjEaYmLULDOZjL8ZMxMzyUxmSeISI+4aR41LIjH+kicqBhQVGkWQtZElzdrd9ELvS9X5/VEFNthAA919u6o+r+epx7q3LnW/l4ufPn3uueeacw4REYl/Pq8LEBGRgaFAFxFJEAp0EZEEoUAXEUkQCnQRkQQR8GrHhYWFrry83Kvdi4jEpWXLltU554r6+syzQC8vL6eystKr3YuIxCUz23Kwz9TlIiKSIBToIiIJQoEuIpIgFOgiIglCgS4ikiAU6CIiCUKBLiKSIOIu0NftbOa//ryOhtYur0sRERlW4i7QN9W1cs+CDWxvave6FBGRYSXuAr0gMwRAQ2u3x5WIiAwvcRfoeenRQK9vU5eLiEhvcRfo+Rl7W+gKdBGR3uIu0HPSgphBvQJdRGQ/cRfofp+RmxZUoIuIHCDuAh0gLyOkPnQRkQPEZaDnp4fUhy4icoC4DPS8jJC6XEREDhCXgZ6fHqJBXS4iIvuJy0DPywjR0NqNc87rUkREho24DPT8jCBd4QitXWGvSxERGTbiMtD33i2qC6MiIh+Jy0DfO5+LLoyKiHwkLgN933wuCnQRkX3iMtD3zueiQBcR+UhcBnre3gm6NHRRRGSfuAz0rJQAAZ+phS4i0ktcBrqZRceiq4UuIrJPXAY6RO8WVQtdROQjcRvoeRlBPYZORKSXuA30fE2hKyKyn7gN9DxNoSsisp+4DfT82EXRSEQTdImIQD8C3cxGm9kCM1ttZqvM7LY+tjEzu8vMNpjZCjObPjjlfiQvPUTEQVO7+tFFRKB/LfQe4HvOuSnALOBmM5tywDYXARNjr7nAfQNaZR/2zeeifnQREaAfge6c2+Gcezf2vhlYA5QesNllwOMu6m0g18xGDni1vWjGRRGR/R1RH7qZlQOnAu8c8FEpUN1reSsfD33MbK6ZVZpZZW1t7ZFVegDN5yIisr9+B7qZZQLPA992zu05mp055+Y55yqccxVFRUVH8xX7aD4XEZH99SvQzSxINMyfdM690Mcm24DRvZbLYusGTf6+KXR1UVREBPo3ysWAh4A1zrn/Ochm84HZsdEus4Am59yOAazzY9JCflKDPrXQRURiAv3Y5izgy8BKM1seW3c7MAbAOfdr4GXgYmAD0AZcP/ClfpzmcxER+chhA9059wZgh9nGATcPVFH9lZehQBcR2Stu7xSF2HwuCnQRESDOAz0vXXOii4jsFdeBrha6iMhH4jrQ89JDNHf00B2OeF2KiIjn4jrQ8zN1c5GIyF7xHej75nPRzUUiInEd6IWxFvquPR0eVyIi4r24DvTjijMBqKpp8bgSERHvxXWgF2SEyEsPUrWr2etSREQ8F9eBbmZMLMlivQJdRCS+Ax3g+JJMqna1EJ19QEQkeSVAoGfR3NnDTl0YFZEkF/eBPrE4C4D1u3RhVESSW9wH+vElsZEu6kcXkSQX94FekJlCQUZIF0ZFJOnFfaADTCzJVJeLiCS9hAj040uy2FCjkS4iktwSItAnlmTR0tnD9iaNdBGR5JUQgX58bAoA9aOLSDJLjEAviQ5d1EgXEUlmCRHoeRkhCjNTdGFURJJaQgQ67J0CQC10EUleCRToWVTVtBCJaKSLiCSnhAn0iSWZtHWF2dbY7nUpIiKeSJhAnxS7MLpmxx6PKxER8UbCBPpJpTmkBf0sqqrzuhQREU8kTKCnBv2cPbGQ19bW6I5REUlKCRPoAOefUMy2xnbW7tRoFxFJPgkV6OedUAzAa2trPK5ERGToJVSgF2enMq0sh1fX7PK6FBGRIZdQgQ7RVvp71Y3sbun0uhQRkSGVcIF+/gklOAevr6v1uhQRkSGVcIF+4qhsirNSeHWtul1EJLkkXKD7fMb5k4tZuL6Orp6I1+WIiAyZwwa6mT1sZjVm9sFBPj/HzJrMbHnsdcfAl3lkzjuhhJbOHpZsqve6FBGRIdOfFvqjwIWH2WaRc+6U2OtHx17WsTl7QiEZIT8vLt/mdSkiIkPmsIHunFsIxFVTNy3k55Jpo/jjyh20dvZ4XY6IyJAYqD70M8zsfTP7f2Z24sE2MrO5ZlZpZpW1tYM7CuVLp5XR1hXmjyt3DOp+RESGi4EI9HeBsc65k4G7gd8fbEPn3DznXIVzrqKoqGgAdn1w08fkMb4og2crqwd1PyIiw8UxB7pzbo9zriX2/mUgaGaFx1zZMTIzvjhjNEs3N7CxVo+mE5HEd8yBbmYjzMxi72fGvnP3sX7vQLhieil+n/Hcsq1elyIiMuj6M2zxKeAtYJKZbTWzG8zsJjO7KbbJlcAHZvY+cBdwtRsm89cWZ6fyqeOLeP7drYT1aDoRSXCBw23gnLvmMJ/fA9wzYBUNsC9VlHHTb2pYuL6Wc2OzMYqIJKKEu1P0QOedUEJRVgoPv7nJ61JERAZVwgd6KODj+rPKWVRVx6rtTV6XIyIyaBI+0AGuO30sGSE/8xZu9LoUEZFBkxSBnpMW5NrTx/DSih1sbWjzuhwRkUGRFIEO8NWzx2HAQ2+oL11EElPSBPrInDQuO6WUp5dU09Da5XU5IiIDLmkCHWDuJ8fT3h3WiBcRSUhJFeiTRmRx6cmjuH/hRrbsbvW6HBGRAZVUgQ7wT5+dTMjv44fzVzFMbmgVERkQSRfoJdmpfOeC43l9XS1/XqXnjopI4ki6QAeYc8ZYThiRxY/+sIq2Lj0AQ0QSQ1IGesDv48efP4ntTR3c/doGr8sRERkQSRnoABXl+VwxvYyHFm1iU50ukIpI/EvaQAf4hwsnEQr4+PFLq70uRUTkmCV1oBdnp3LLeRN4dW0NC9bVeF2OiMgxSepAB7j+rHGML8zgzj+spqsn4nU5IiJHLekDPRTw8c+XTmFjXSuPLtYdpCISv5I+0AHOnVTMpycX8/O/VFFdr9kYRSQ+KdBjfnTZSfh9xu2/W6k7SEUkLinQY0blpvEPF05iUVUdz7+7zetyRESOmAK9l+tOH0vF2DzufGk1tc2dXpcjInJEFOi9+HzGv18xjfauMP/6h1VelyMickQU6AeYUJzJzedO4KUVO/jr+lqvyxER6TcFeh9uOmc84wszuOPFD+joDntdjohIvyjQ+5AS8PPjz5/Elt1t/GqBJu8SkfigQD+IMycU8oVTS7nvrx+yoabF63JERA5LgX4It188mbSgn9tfWEk4orHpIjK8KdAPoSgrhTsuPZElm+u569Uqr8sRETkkBfphXDmjjMunl3LXa1Us3lDndTkiIgelQO+HOy87ifGFGdz2zHLdcCQiw5YCvR8yUgLce9109rR3893fLiei/nQRGYYU6P10wohs7rh0Couq6njynS1elyMi8jEK9CNw7cwxfPL4In7y8lo26zmkIjLMKNCPgJnxH1dMJeA3/v659zWUUUSGlcMGupk9bGY1ZvbBQT43M7vLzDaY2Qozmz7wZQ4fI3PS+JdLT2Tp5gYeeVNPOBKR4aM/LfRHgQsP8flFwMTYay5w37GXNbxdPr2UC6aU8LM/rWN5daPX5YiIAP0IdOfcQqD+EJtcBjzuot4Gcs1s5EAVOBxFu16mUZydwk1PLKOmucPrkkREBqQPvRSo7rW8NbbuY8xsrplVmlllbW18T02bnxFi3pcraGrv5hu/eZeunojXJYlIkhvSi6LOuXnOuQrnXEVRUdFQ7npQTBmVzX9+cRrLtjTww/mr9CxSEfFUYAC+YxswutdyWWxdUrhk2ihWbd/Dfa9/yPElmVx/1jivSxKRJDUQLfT5wOzYaJdZQJNzbscAfG/c+LvPTOKCKSXc+dJqFqyt8bocEUlS/Rm2+BTwFjDJzLaa2Q1mdpOZ3RTb5GVgI7ABeAD45qBVO0z5fcYvrz6FySOzueWp91i7c4/XJYlIEjKv+n0rKipcZWWlJ/seLDubOrjs3jcI+Hy8+K2zKMxM8bokEUkwZrbMOVfR12e6U3QAjchJ5cHZp1HX0snNT75Ld1gjX0Rk6CjQB9jUshx+evlU3tlUz09eXuN1OSKSRAZilIsc4PLpZazc1sQjb25mamkOl08v87okEUkCaqEPktsvnsys8fn84wsrWbm1yetyRCQJKNAHSdDv495rp1OYmcLXHq+kZo+mBxCRwaVAH0QFmSk8MDs6PcDcJ5bR0R32uiQRSWAK9EE2ZVQ2P7/qZJZXN3L771ZqegARGTQK9CFw4Ukj+c6nj+eFd7fxy1ervC5HRBKURrkMkVvPn8DWhjZ+8UoV+RkhZp9R7nVJIpJgFOhDxMz46eVTaWzv5ofzV5GTFuSyU/qcZVhE5Kioy2UIBfw+7r7mVGaW5/O9377Poqr4nhNeRIYXBfoQSw36eWBOBROKM/nmk+/yYW2L1yWJSIJQoHsgOzXIA7MrCPl93PhYJY1tXV6XJCIJQIHukdH56dz/5Rlsa2jnm5rIS0QGgALdQxXl+fzk8qks/nA3t/zve7rxSESOiQLdY1fOKOOfL5nCn1bt5PpHltLc0e11SSISpxTow8ANZ4/j51edzNLN9VzzwNvUNnd6XZKIxCEF+jDxhVPLeGB2BRtqWvjirxdTXd/mdUkiEmcU6MPIuScU8+SNs2ho6+by+xazerueTSoi/adAH2ZmjM3juZvOIOAzrrr/LZZurve6JBGJEwr0YWhiSRbPf+NMirJT+OqjS1m3s9nrkkQkDijQh6lRuWk8ccPppIf8zHl4Cdsb270uSUSGOQX6MFaam8aj18+ktbOHrzyyhKY2DWkUkYNToA9zk0dmc//sGWyqa2X2I0toaNU0ASLSNwV6HDjzuEJ+dd0M1uzYw1Xz3mKXnk8qIn1QoMeJC6aU8Oj1p7GtoZ0r7lvM5rpWr0sSkWFGgR5HzjyukKfmzqK1s4crf72Y96sbvS5JRIYRBXqcmVaWy3PfOJPUoJ+r573Na2t3eV2SiAwTCvQ4dFxRJi9880wmFGdy42OVPL3kb16XJCLDgAI9ThVnpfL03Fl8YmIRP3hhJY8t3ux1SSLiMQV6HMtICTBv9gwumFLCD+ev4qE3Nnldkoh4SIEe51ICfu69djoXnjiCO19azYOLNnpdkoh4RIGeAEIBH3dfeyoXTx3Bj/+4ht+9t9XrkkTEA/0KdDO70MzWmdkGM/tBH59/xcxqzWx57HXjwJcqhxL0+/j5Vacwa3w+339uBYs/rPO6JBEZYocNdDPzA/cCFwFTgGvMbEofmz7jnDsl9npwgOuUfkgJ+Ln/yxWUF2Tw9SeWsX6XZmkUSSb9aaHPBDY45zY657qAp4HLBrcsOVo5aUEe/epM0mLj1B96Y5MePi2SJPoT6KVAda/lrbF1B7rCzFaY2XNmNnpAqpOjUpqbxpM3ns6kkizufGk1n/zZAn7z9hacc16XJiKDaKAuiv4BKHfOTQP+AjzW10ZmNtfMKs2ssra2doB2LX2ZWJLFU3Nn8dTXZlFekME//f4DfvD8SrrDEa9LE5FB0p9A3wb0bnGXxdbt45zb7Zzb+6j6B4EZfX2Rc26ec67COVdRVFR0NPXKETrjuAKe+fosbj1vAs9UVvO1xytp7ezxuiwRGQT9CfSlwEQzG2dmIeBqYH7vDcxsZK/FzwFrBq5EOVZmxnc/M4mfXj6VRVV1XD3vbWqbOw//B0Ukrhw20J1zPcC3gD8TDerfOudWmdmPzOxzsc1uNbNVZvY+cCvwlcEqWI7eNTPH8MDsGVTVNPPFXy+mur7N65JEZACZVxfKKioqXGVlpSf7TnbLtjTw1UeXEgr4eOz6mUwZle11SSLST2a2zDlX0ddnulM0Cc0Ym8dzN51BwGd86f63ePytzfToYqlI3FOgJ6mJJVk8/40zmVqawx0vruKSu9/g7Y27vS5LRI6BAj2JjcpN43+/djq/um46zR09XD3vba5/ZAkrtupJSCLxSIGe5MyMi6eO5JXvfoq//z+TeK+6kc/d8yY3PrZUF01F4owCXQBIC/m5+dwJLPr+ufzdZ47nnY31XHrPGyyq0g1gIvFCgS77yUoN8q3zJjL/lrMpyUplzsNLuHfBBiIRTRsgMtwp0KVP4woz+N3NZ/LZaaP4zz+v47z/fp0HF22ksa3L69JE5CA0Dl0OyTnHSyt28NjizVRuaSAl4OOamWO4+dwJFGWleF2eSNI51Dh0Bbr02+rte3h08Saef3cbKQEfXz1rHF//1HiyUoNelyaSNHRjkQyIKaOy+dmVJ/OX73yS8yeXcM+CDVx69xus3bnH69JEBAW6HIXxRZncfc2pPHvTGbR1hfnCvYt5cfm2w/9BERlUCnQ5aqeV5/PSrWcztTSH255ezi1Pvcfq7Wqti3hFgS7HpDgrlSe/djrfOncCr63ZxcV3LWLOw0t4fV0NYQ11FBlSuigqA6aprZvfvLOFR97cRF1LF8VZKXz+1FKuPm0044syvS5PJCFolIsMqc6eMAvW1vDcsm28vq4GB3x51lhuO38ieRkhr8sTiWsKdPFMTXMHv3yliqeW/I2s1CC3nDeBa2aOISMl4HVpInFJgS6eW7tzD//2xzUsqqojNz3InDPKufb0MRRnpWBmXpcnEjcU6DJsLNtSz32vb+SVNbsASAv6GZWbyqQRWcw5o5yZ4/IV8CKHcKhA1++9MqRmjM3nwTn5VO1q5q/ra9nR1MH2xnbe2VjPyyt3csroXL5yZjmTR2YztiCd1KDf65JF4oYCXTwxsSSLiSVZ+5Y7usM8u2wrDyzcyLefWb5vfVleGuedUMyFJ41gZnk+Ab9G2oocjLpcZFgJRxyrtjexeXcbW+paWbmtiYVVtXR0RyjICDHnzHLmnFlOTprmj5HkpD50iWttXT38dV0tzy7bymtra8hMCXDdrDHMGJPH6Px0yvLSNEGYJA31oUtcSw8FuGjqSC6aOpLV2/dw7+sbmLdwI73bItmpAUrz0inNTePSk0dyybRR+H26uCrJRS10iUuNbV1s2d3G1oZ2qhva2N7YzvbGdtbvauFv9W0cV5TBredP5Jzji8lOC2jkjCQMtdAl4eSmh8hND3Hy6Nz91kcijj+t2skvXlnPbU9HL64G/UZeeojSvDTGFWYwriCD9JQAHd1hOrvDjC/K5OKpIwkFdMFV4pta6JKQIhHH6+tr2Fjbyu7WLna3dFJd386mulZ27un42PYl2Sl85cxxXDJtJNmpQTJS/BpRI8OSLoqK9NLW1UNXT4TUoJ+Q38fCqloeWLSRNzfs3m+7wswUZo7L4/RxBUwsyaSjO0xLZ5igzzjjuAJy0zUvjQw9dbmI9JIeCtA7i8+ZVMw5k4pZs2MP71c30tLZQ2tnmM27W1myKXrD04H8PmPG2Dw+MaGQgswUMlMD5KYFGVeYQWluGr7YBdlwxNHc0U12anDfOpHBokAXiZk8MpvJI7M/tn5rQxtbdreRkRIgM8VPU3sPC9bW8MqaXfz3X9Z/bPvUoI+yvHT2tHezu7WLcMQRCvgYm5/O2IIMTirN5pTRuZwyOletfBlQ6nIROQatnT00d/TQ0tnN7pYuNta1sqGmha0NbeSkBSnOSiU3PUhNcyeb61rZWNfKh7Ut+4ZcZqUGyM8IkZceIug3eiKOSMRRmpfGzPJ8ThuXT156iNrmTmqaOwlHImSnBclNC5GbHiQ/I6TpEZKMulxEBklGSiA2FXAqE4rh9PEFh/0zLZ09rNjayPvVTeza00FDWxf1rV30hB2pwWi3zPvVTX129fQlPeRnTH46n55cwmdOLOGkUTk0tnezvbGd+tYuws7hnCPo9zGtNJecdN2ElajUQhcZprY2tFG5uYG2rjDFWSkUZ6cQ8Ploau+mqb2L+tbufT8MVm1vYunmBsIRR8AXbekfzKSSLKaW5dDW1UNtcydN7d1MKM7k1NF5nDIml9LcNAozU/YN4+zqidDS2UPQb2SmaEy/19RCF4lDZXnplOWl93v7htYuXl1bQ9WuZkqyUxmZk0phVgoBn+Ezo6Wzh3e3NLBkcz1/XV9LdmqAoqwUxuRnsHLbx38jyEoN0B2O0NEd2bfOZ5CdFqS8IINTx0SvA5gZG2pa2FDTTEtnmLSgj7Sgn9Sgn7SQn/SQn66eCNX10ZvA9nR0k5ceIj8jREFGCiNyUhiRncrYggzOmlCoO3yPgVroIgJAbXMnK7c1srOpk7qWTupbuwgFfGSlBMhKDdAVjrCnvYfG9i7W72xhxbbGfWHvMxhbkEF2WpDO7jDt3WHausJ0dIVp6w4T8Bmj89MZnZdGTlqQhrZu6lu7qGvZe20gmkNjC9K58exxXDljNGkhXRvoyzGPQzezC4FfAn7gQefcvx/weQrwODAD2A1c5ZzbfKjvVKCLxLfucISqXS34fFBekHHIi7POuYN21YQjjt0tnSzb0sD9CzeyvLqR7NQAp5Xnc/LoXKaW5TAqJ43CzOjF42Qf/nlMgW5mfmA9cAGwFVgKXOOcW91rm28C05xzN5nZ1cAXnHNXHep7FegiciDnHEs3N/BsZTXvVTfuNyIIouP/i7NSGJGTyojsVLJTg6Sn+MlMCZAW8pMaiHbzhPw+An4j5Pfh99m+VygQ7Q5KDwXw+4xwxNETieAcmIHPjKDfR3ZqgOy0ICkB37C7ZnCsfegzgQ3OuY2xL3sauAxY3Wuby4B/ib1/DrjHzMx51Z8jInHJzJg5Lp+Z4/IB2NPRzdodzdQ0d1DX3EltSyc7mzrZtaeDqpoWmju6ae0M09rVw2Ckjc+iNRnRsDeL/lDxm+GI/gCC6LqA34fPjIhzdIcjRGL3H6QG/aQF/fv9ZnH1aaO58RPjB7ze/gR6KVDda3krcPrBtnHO9ZhZE1AA1A1EkSKSnLJTg/vC/VCcc3T2ROiI9d939UToDkeDNRxxsZa4o6snQnt3D+1dEXoiEYKxFrzBvoDu7Imwp6OH5o5u2jrDOBzOQcRFPw9HHGHnMKIBD+y3j0Cv3wi6wxHau6I19f6BU5iZMih/X0M6ysXM5gJzAcaMGTOUuxaRBGZmpMZG1uQefvOE1Z/p5LYBo3stl8XW9bmNmQWAHKIXR/fjnJvnnKtwzlUUFRUdXcUiItKn/gT6UmCimY0zsxBwNTD/gG3mA3Ni768EXlP/uYjI0Dpsl0usT/xbwJ+JDlt82Dm3ysx+BFQ65+YDDwFPmNkGoJ5o6IuIyBDqVx+6c+5l4OUD1t3R630H8MWBLU1ERI6EHskiIpIgFOgiIglCgS4ikiAU6CIiCcKz2RbNrBbYcpR/vJDkvAs1GY87GY8ZkvO4k/GY4ciPe6xzrs8beTwL9GNhZpUHm5wmkSXjcSfjMUNyHncyHjMM7HGry0VEJEEo0EVEEkS8Bvo8rwvwSDIedzIeMyTncSfjMcMAHndc9qGLiMjHxWsLXUREDqBAFxFJEHEX6GZ2oZmtM7MNZvYDr+sZDGY22swWmNlqM1tlZrfF1ueb2V/MrCr23zyvax0MZuY3s/fM7KXY8jgzeyd2zp+JTeOcMMws18yeM7O1ZrbGzM5IhnNtZt+J/fv+wMyeMrPURDzXZvawmdWY2Qe91vV5fi3qrtjxrzCz6Ueyr7gK9NgDq+8FLgKmANeY2RRvqxoUPcD3nHNTgFnAzbHj/AHwqnNuIvBqbDkR3Qas6bX8H8DPnXMTgAbgBk+qGjy/BP7knDsBOJnosSf0uTazUuBWoMI5dxLRqbmvJjHP9aPAhQesO9j5vQiYGHvNBe47kh3FVaDT64HVzrkuYO8DqxOKc26Hc+7d2Ptmov+DlxI91sdimz0GfN6bCgePmZUBnwUejC0bcB7Rh49Dgh23meUAnyT6TAGcc13OuUaS4FwTnb47LfaUs3RgBwl4rp1zC4k+J6K3g53fy4DHXdTbQK6ZjezvvuIt0Pt6YHWpR7UMCTMrB04F3gFKnHM7Yh/tBEo8Kmsw/QL4PhCJLRcAjc65nthyop3zcUAt8Eism+lBM8sgwc+1c24b8F/A34gGeROwjMQ+170d7PweU8bFW6AnFTPLBJ4Hvu2c29P7s9gj/hJqzKmZXQLUOOeWeV3LEAoA04H7nHOnAq0c0L2SoOc6j2hrdBwwCsjg490SSWEgz2+8BXp/HlidEMwsSDTMn3TOvRBbvWvvr1+x/9Z4Vd8gOQv4nJltJtqddh7R/uXc2K/lkHjnfCuw1Tn3Tmz5OaIBn+jn+tPAJudcrXOuG3iB6PlP5HPd28HO7zFlXLwFen8eWB33Yv3GDwFrnHP/0+uj3g/jngO8ONS1DSbn3D8658qcc+VEz+1rzrnrgAVEHz4OCXbczrmdQLWZTYqtOh9YTYKfa6JdLbPMLD32733vcSfsuT7Awc7vfGB2bLTLLKCpV9fM4Tnn4uoFXAysBz4E/q/X9QzSMZ5N9FewFcDy2Otiov3JrwJVwCtAvte1DuLfwTnAS7H344ElwAbgWSDF6/oG+FhPASpj5/v3QF4ynGvgX4G1wAfAE0BKIp5r4Cmi1wm6if5GdsPBzi9gREfyfQisJDoKqN/70q3/IiIJIt66XERE5CAU6CIiCUKBLiKSIBToIiIJQoEuIpIgFOgiIglCgS4ikiD+Py74W/93S1IgAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "bruFCYXs7DzO",
        "outputId": "ae02d4ee-1e27-4f4b-a4fb-887452c0ba45"
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5e49c4de50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "zj-5YEqBeu0W",
        "outputId": "0a038b77-8388-4ab4-924a-994bc25ffb7b"
      },
      "source": [
        "plt.plot(accs_old)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5e40fb1d90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AzTrBxn47GVb",
        "outputId": "c7099899-c0c4-4860-bbb7-03a3ef48874e"
      },
      "source": [
        "plt.plot(accs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5e404d0850>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUwg4bDL7XM7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}